---
title: Notas sobre inteligencia artificial (AI)
tags:
    - ia
    - machine-lerning
    - mllm
---

## Reglamento europeo de Inteligencia Artificial

Esta norma, también conocida como **Ley Europea de Inteligencia
Artificial**, constituye la primera regulación general a nivel mundial y
con rango de Ley de esta importante materia, y condicionará el
desarrollo económico y social de los próximos años.

La UE se sitúa así a la vanguardia de la regulación de la IA. La otra
regulación general importante, en Estados Unidos, constituida por la
orden ejecutiva del presidente Joe Biden de 30 de octubre de 2023, es
mucho menos intensa en su carga obligacional y no proviene de una fuente
parlamentaria.

La regulación europea no sólo contempla mecanismos de control y
regulación, sino también, y es importante destacarlo, de fomento (como
la regulación de los espacios controlados de pruebas -_sandboxes_- y las
medidas a favor del desarrollo por las pymes de sistemas de IA)
destinadas a incentivar un desarrollo de estas tecnologías sostenible en
términos sociales.

## Entrada en vigor de la Ley Europea de Inteligencia Artificial

La publicación del texto definitivo en el DOUE pone en marcha el
mecanismo para su plena entrada en vigor. El reglamento no será
aplicable con carácter general hasta el 2 de agosto de 2026, pasados 24
meses de su entrada en vigor. No obstante, algunas previsiones del mismo
son aplicables en plazos distintos:

- Las **prohibiciones** de determinadas prácticas relacionadas con la IA
  serán ya aplicables a partir del **2 de febrero de 2025**.

- Las previsiones relativas a los **organismos notificados**, a los
  sistemas de IA generales pero que implican **riesgos sistémicos**, al
  **sistema de gobernanza de la IA** en Europa y buena parte del arsenal
  **sancionador** serán aplicables a partir del **2 de agosto de 2025**
  (con lo que la base organizativa estará ya lista para cuando sea
  exigible el conjunto más sustancial de obligaciones).

- Por último, será aplicable a partir del **2 de agosto de 2027** la
  regulación de **ciertos sistemas de IA de alto riesgo** (los que sean
  componentes de seguridad de ciertos productos o constituyan en sí
  mismos dichos productos caracterizados por requerirse una evaluación
  de seguridad para su comercialización o puesta en servicio -por
  ejemplo, máquinas, juguetes, ascensores o productos sanitarios-).

## Contenido básico del reglamento: mecanismos de control y regulación

En el ámbito de control y regulación, el Reglamento de IA clasifica los
sistemas de inteligencia artificial en **función del riesgo** que pueden
generar y según sus usos.

Establece **cuatro niveles de riesgo**:

- Riesgo **inadmisible**: se aplica esta categoría a un conjunto muy
  limitado de prácticas de IA especialmente nocivas que se oponen a los
  valores de la UE porque **vulneran los derechos fundamentales** y, en
  consecuencia, quedan prohibidas. Estos usos o prácticas prohibidas
  (determinados en el artículo 5 del reglamento) incluyen, por ejemplo,
  la puntuación social con fines públicos y privados, el uso de técnicas
  subliminales, eI aprovechamiento de los puntos vulnerables de las
  personas, la categorización biométrica de las personas físicas, o el
  reconocimiento de emociones en el lugar de trabajo y en las
  instituciones educativas (a menos que sea por razones médicas o de
  seguridad)

- **Alto riesgo**: se clasifican como de alto riesgo un número limitado
  de sistemas de inteligencia artificial definidos en el reglamento y
  que se incluyen, a su vez, en dos categorías:

  - Productos o componentes de seguridad de productos que según la
    normativa recogida en el anexo I tienen que someterse a una
    evaluación de conformidad con la normativa UE realizada por terceros
    antes de su comercialización o puesta en servicio (lo que incluye
    máquinas, juguetes, ascensores, productos sanitarios o vehículos de
    motos, entre otros)

  - Diversos sistemas enumerados en el anexo III, que incluye tanto
    sistemas a utilizar por las Administraciones Públicas, por ejemplo,
    en el ámbito de las políticas sociales o de extranjería, como en el
    ámbito privado, como en los seguros de vida y salud, la
    clasificación crediticia de personas físicas, o en la selección,
    promoción o despido de personal, y que tienen en general un impacto
    potencial negativo en la seguridad de las personas o en sus derechos
    fundamentales, tal y como están protegidos por la Carta de los
    Derechos Fundamentales de la UE.

Estos sistemas quedan sometidos a una intensa regulación, que afecta a
sus proveedores, pero también en diversa medida a los demás agentes
intervinientes en su cadena de valor (representantes autorizados,
importadores, distribuidores, responsables del despliegue) de forma que
no sea posible eludir la responsabilidad.

En primer lugar, deben someterse a una evaluación de la conformidad con
los requisitos obligatorios de una inteligencia artificial digna de
confianza (por ejemplo, calidad de los datos, documentación y
trazabilidad, transparencia, supervisión humana, exactitud,
ciberseguridad y solidez -resistencia a errores, fallos e
incoherencias-). Esta evaluación se puede hacer por el propio proveedor
si se atiene a normas armonizadas o especificaciones comunes
establecidas por la UE, o, en otro caso, por un organismo notificado
conforme al procedimiento previsto en el anexo VII. Una vez realizada la
evaluación, si es positiva, se declara la conformidad del sistema con el
reglamento (Declaración UE de conformidad, que el proveedor deberá
facilitar a las autoridades de supervisión si se lo solicitan) y se
procede a su Marcado CE de conformidad. Los sistemas de alto riesgo se
inscriben también en un registro público (excepto cuando los usan los
poderes públicos para fines policiales o de migración, en cuyo caso el
registro es de acceso restringido por razones obvias).

También se tendrán que aplicar sistemas de gestión de la calidad y de
los riesgos, incluso después de que los productos se hayan
comercializado.

- **Riesgo bajo**: los demás sistemas de inteligencia artificial pueden,
  en principio, desarrollarse y utilizarse con arreglo a la legislación
  vigente, sometidos a un régimen relativamente sencillo de obligaciones
  de información y de respeto a los derechos de propiedad intelectual,
  de autor y similares (que impone el artículo 53). **La inmensa mayoría
  de los sistemas de inteligencia artificial utilizados actualmente o
  que se prevee se van a utilizar en un futuro pertenecen a esta
  categoría**. En estos casos, de forma voluntaria, los proveedores de
  estos sistemas pueden optar por adherirse a códigos de conducta
  facultativos, o demostrar que cumplen sus obligaciones de
  transparencia y respeto a los derechos de propiedad intelectual por
  otras vías bajo control de la Comisión Europea.

Por excepción, sin embargo, se imponen obligaciones específicas de
transparencia (artículo 50) a determinados sistemas de IA de bajo riesgo
cuando existe un peligro claro de confusión o manipulación del usuario
(por ejemplo, mediante el uso de robots conversacionales o cuando se
haga uso de técnicas de ultrasuplantación, de forma que contenidos
obtenidos por IA puedan inducir a una persona a pensar erróneamente que
son auténticos o verídicos). En estos casos, el reglamento exige, por
ejemplo, que se **asegure que los usuarios puedan saber que están
interactuando con una máquina o que los contenidos a los que se ven
expuestos han sido generados o manipulados de manera artificial**.

Por último, para los sistemas generales de IA, el reglamento tiene en
cuenta los riesgos sistémicos que podrían derivarse de su uso, incluido
el de los grandes modelos generativos de inteligencia artificial. Estos
sistemas que pueden utilizarse para diversas tareas podrían entrañar
riesgos sistémicos si tienen capacidades de gran impacto o de impacto
equivalente (se considera que los modelos de inteligencia artificial de
uso general entrenados con una potencia informática total de más de
10^25 FLOPS -ChatGPT 4 o GEMINI- entrañan riesgos sistémicos). Dado que
estos modelos potentes podrían causar accidentes graves o ser utilizados
indebidamente para ciberataques de amplio alcance, el reglamento impone
obligaciones adicionales de evaluación y mitigación del riesgo, de
comunicación en caso de incidentes, y de protección de la ciberseguridad
(artículo 55), pudiendo los proveedores de estos sistemas acudir también
a códigos de buenas prácticas para demostrar el cumplimiento de estas
obligaciones.

## Estructura administrativa de control y régimen sancionador

El reglamento obliga a los Estados miembros a fijar uno o varios órganos
competentes para la supervisión del cumplimiento de las obligaciones que
impone.

En España ya se ha creado, por el Real Decreto 729/2023, de 22 de
agosto, la **Agencia Española de Supervisión de Inteligencia
Artificial** ([AESIA](https://aesia.digital.gob.es/es)), que ejerce las
funciones de inspección, comprobación, y sanción de conformidad con el
Reglamento de IA. En definitiva, será la **principal autoridad
supervisora** nacional.

A nivel europeo, la **Oficina Europea de Inteligencia Artificial**,
creada por la Decisión de la Comisión de 24 de enero de 2024
(DOUE-Z-2024-70007), será el organismo de control al que corresponderán
importantes funciones, especialmente en la supervisión de los modelos de
IA de uso general

Las multas por infracciones del Reglamento de Inteligencia Artificial se
han fijado como un **porcentaje del volumen de negocios anual global**
de la empresa infractora en el ejercicio financiero anterior o un
importe predeterminado, si este fuera superior, y pueden llegar hasta
los **35 millones de euros** o el **7% del volumen de negocios anual
total a escala mundial** del infractor durante el ejercicio financiero
anterior, si este importe fuera superior.
