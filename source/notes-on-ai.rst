Inteligencia Artificial (AI)
=======================================================================

.. tags: ia, machine-learning, law, science

Reglamento europeo de Inteligencia Artificial
------------------------------------------------------------------------

Esta norma, también conocida como **Ley Europea de Inteligencia
Artificial**, constituye la primera regulación general a nivel mundial y
con rango de Ley de esta importante materia, y condicionará el
desarrollo económico y social de los próximos años.

La UE se sitúa así a la vanguardia de la regulación de la IA. La otra
regulación general importante, en Estados Unidos, constituida por la
orden ejecutiva del presidente Joe Biden de 30 de octubre de 2023, es
mucho menos intensa en su carga obligacional y no proviene de una fuente
parlamentaria.

La regulación europea no sólo contempla mecanismos de control y
regulación, sino también, y es importante destacarlo, de fomento (como
la regulación de los espacios controlados de pruebas -*sandboxes*- y las
medidas a favor del desarrollo por las pymes de sistemas de IA)
destinadas a incentivar un desarrollo de estas tecnologías sostenible en
términos sociales.

Entrada en vigor de la Ley Europea de Inteligencia Artificial
------------------------------------------------------------------------

La publicación del texto definitivo en el DOUE pone en marcha el
mecanismo para su plena entrada en vigor. El reglamento no será
aplicable con carácter general hasta el 2 de agosto de 2026, pasados 24
meses de su entrada en vigor. No obstante, algunas previsiones del mismo
son aplicables en plazos distintos:

-  Las **prohibiciones** de determinadas prácticas relacionadas con la
   IA serán ya aplicables a partir del **2 de febrero de 2025**.

-  Las previsiones relativas a los **organismos notificados**, a los
   sistemas de IA generales pero que implican **riesgos sistémicos**, al
   **sistema de gobernanza de la IA** en Europa y buena parte del
   arsenal **sancionador** serán aplicables a partir del **2 de agosto
   de 2025** (con lo que la base organizativa estará ya lista para
   cuando sea exigible el conjunto más sustancial de obligaciones).

-  Por último, será aplicable a partir del **2 de agosto de 2027** la
   regulación de **ciertos sistemas de IA de alto riesgo** (los que sean
   componentes de seguridad de ciertos productos o constituyan en sí
   mismos dichos productos caracterizados por requerirse una evaluación
   de seguridad para su comercialización o puesta en servicio -por
   ejemplo, máquinas, juguetes, ascensores o productos sanitarios-).

Contenido básico del reglamento: mecanismos de control y regulación
------------------------------------------------------------------------

En el ámbito de control y regulación, el Reglamento de IA clasifica los
sistemas de inteligencia artificial en **función del riesgo** que pueden
generar y según sus usos.

Establece **cuatro niveles de riesgo**:

-  Riesgo **inadmisible**: se aplica esta categoría a un conjunto muy
   limitado de prácticas de IA especialmente nocivas que se oponen a los
   valores de la UE porque **vulneran los derechos fundamentales** y, en
   consecuencia, quedan prohibidas. Estos usos o prácticas prohibidas
   (determinados en el artículo 5 del reglamento) incluyen, por ejemplo,
   la puntuación social con fines públicos y privados, el uso de
   técnicas subliminales, eI aprovechamiento de los puntos vulnerables
   de las personas, la categorización biométrica de las personas
   físicas, o el reconocimiento de emociones en el lugar de trabajo y en
   las instituciones educativas (a menos que sea por razones médicas o
   de seguridad)

-  **Alto riesgo**: se clasifican como de alto riesgo un número limitado
   de sistemas de inteligencia artificial definidos en el reglamento y
   que se incluyen, a su vez, en dos categorías:

   -  Productos o componentes de seguridad de productos que según la
      normativa recogida en el anexo I tienen que someterse a una
      evaluación de conformidad con la normativa UE realizada por
      terceros antes de su comercialización o puesta en servicio (lo que
      incluye máquinas, juguetes, ascensores, productos sanitarios o
      vehículos de motos, entre otros)

   -  Diversos sistemas enumerados en el anexo III, que incluye tanto
      sistemas a utilizar por las Administraciones Públicas, por
      ejemplo, en el ámbito de las políticas sociales o de extranjería,
      como en el ámbito privado, como en los seguros de vida y salud, la
      clasificación crediticia de personas físicas, o en la selección,
      promoción o despido de personal, y que tienen en general un
      impacto potencial negativo en la seguridad de las personas o en
      sus derechos fundamentales, tal y como están protegidos por la
      Carta de los Derechos Fundamentales de la UE.

Estos sistemas quedan sometidos a una intensa regulación, que afecta a
sus proveedores, pero también en diversa medida a los demás agentes
intervinientes en su cadena de valor (representantes autorizados,
importadores, distribuidores, responsables del despliegue) de forma que
no sea posible eludir la responsabilidad.

En primer lugar, deben someterse a una evaluación de la conformidad con
los requisitos obligatorios de una inteligencia artificial digna de
confianza (por ejemplo, calidad de los datos, documentación y
trazabilidad, transparencia, supervisión humana, exactitud,
ciberseguridad y solidez -resistencia a errores, fallos e
incoherencias-). Esta evaluación se puede hacer por el propio proveedor
si se atiene a normas armonizadas o especificaciones comunes
establecidas por la UE, o, en otro caso, por un organismo notificado
conforme al procedimiento previsto en el anexo VII. Una vez realizada la
evaluación, si es positiva, se declara la conformidad del sistema con el
reglamento (Declaración UE de conformidad, que el proveedor deberá
facilitar a las autoridades de supervisión si se lo solicitan) y se
procede a su Marcado CE de conformidad. Los sistemas de alto riesgo se
inscriben también en un registro público (excepto cuando los usan los
poderes públicos para fines policiales o de migración, en cuyo caso el
registro es de acceso restringido por razones obvias).

También se tendrán que aplicar sistemas de gestión de la calidad y de
los riesgos, incluso después de que los productos se hayan
comercializado.

-  **Riesgo bajo**: los demás sistemas de inteligencia artificial
   pueden, en principio, desarrollarse y utilizarse con arreglo a la
   legislación vigente, sometidos a un régimen relativamente sencillo de
   obligaciones de información y de respeto a los derechos de propiedad
   intelectual, de autor y similares (que impone el artículo 53). **La
   inmensa mayoría de los sistemas de inteligencia artificial utilizados
   actualmente o que se prevee se van a utilizar en un futuro pertenecen
   a esta categoría**. En estos casos, de forma voluntaria, los
   proveedores de estos sistemas pueden optar por adherirse a códigos de
   conducta facultativos, o demostrar que cumplen sus obligaciones de
   transparencia y respeto a los derechos de propiedad intelectual por
   otras vías bajo control de la Comisión Europea.

Por excepción, sin embargo, se imponen obligaciones específicas de
transparencia (artículo 50) a determinados sistemas de IA de bajo riesgo
cuando existe un peligro claro de confusión o manipulación del usuario
(por ejemplo, mediante el uso de robots conversacionales o cuando se
haga uso de técnicas de ultrasuplantación, de forma que contenidos
obtenidos por IA puedan inducir a una persona a pensar erróneamente que
son auténticos o verídicos). En estos casos, el reglamento exige, por
ejemplo, que se **asegure que los usuarios puedan saber que están
interactuando con una máquina o que los contenidos a los que se ven
expuestos han sido generados o manipulados de manera artificial**.

Por último, para los sistemas generales de IA, el reglamento tiene en
cuenta los riesgos sistémicos que podrían derivarse de su uso, incluido
el de los grandes modelos generativos de inteligencia artificial. Estos
sistemas que pueden utilizarse para diversas tareas podrían entrañar
riesgos sistémicos si tienen capacidades de gran impacto o de impacto
equivalente (se considera que los modelos de inteligencia artificial de
uso general entrenados con una potencia informática total de más de
10^25 FLOPS -ChatGPT 4 o GEMINI- entrañan riesgos sistémicos). Dado que
estos modelos potentes podrían causar accidentes graves o ser utilizados
indebidamente para ciberataques de amplio alcance, el reglamento impone
obligaciones adicionales de evaluación y mitigación del riesgo, de
comunicación en caso de incidentes, y de protección de la ciberseguridad
(artículo 55), pudiendo los proveedores de estos sistemas acudir también
a códigos de buenas prácticas para demostrar el cumplimiento de estas
obligaciones.

Estructura administrativa de control y régimen sancionador
------------------------------------------------------------------------

El reglamento obliga a los Estados miembros a fijar uno o varios órganos
competentes para la supervisión del cumplimiento de las obligaciones que
impone.

En España ya se ha creado, por el Real Decreto 729/2023, de 22 de
agosto, la **Agencia Española de Supervisión de Inteligencia
Artificial** (`AESIA <https://aesia.digital.gob.es/es>`__), que ejerce
las funciones de inspección, comprobación, y sanción de conformidad con
el Reglamento de IA. En definitiva, será la **principal autoridad
supervisora** nacional.

A nivel europeo, la **Oficina Europea de Inteligencia Artificial**,
creada por la Decisión de la Comisión de 24 de enero de 2024
(DOUE-Z-2024-70007), será el organismo de control al que corresponderán
importantes funciones, especialmente en la supervisión de los modelos de
IA de uso general

Las multas por infracciones del Reglamento de Inteligencia Artificial se
han fijado como un **porcentaje del volumen de negocios anual global**
de la empresa infractora en el ejercicio financiero anterior o un
importe predeterminado, si este fuera superior, y pueden llegar hasta
los **35 millones de euros** o el **7% del volumen de negocios anual
total a escala mundial** del infractor durante el ejercicio financiero
anterior, si este importe fuera superior.
